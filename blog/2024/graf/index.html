<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>GRAF -- neural graph features for performance prediction | Gabi Kadlecová</title> <meta name="author" content="Gabi Kadlecová"> <meta name="description" content="A short overview of our ICML '24 paper -- we show how to do interpretable performance prediction that outperforms complex SOTA predictors like graph neural networks."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, giscus"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%98%95&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gabikadlecova.github.io/blog/2024/graf/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Gabi </span>Kadlecová</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">GRAF -- neural graph features for performance prediction</h1> <p class="post-meta">June 25, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/research"> <i class="fas fa-hashtag fa-sm"></i> research</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>In our ICML paper <a href="https://openreview.net/forum?id=EhPpZV6KLk" rel="external nofollow noopener" target="_blank">Surprisingly Strong Performance Prediction with Neural Graph Features</a>, we introduced neural graph features (GRAF) – simple features such as path lengths or operation counts that outperform existing complex predictors.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_graf/GRAF_schema-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_graf/GRAF_schema-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_graf/GRAF_schema-1400.webp"></source> <img src="/assets/img/blog_graf/GRAF_schema.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="code">Code</h3> <p>To reproduce experiments from the paper, use the repo <a href="https://github.com/gabikadlecova/zc_combine/" rel="external nofollow noopener" target="_blank">zc_combine</a>. We also provide a refactored version of the code for practical use in the repo <a href="https://github.com/gabikadlecova/GRAF/" rel="external nofollow noopener" target="_blank">GRAF</a>.</p> <h3 id="how-to-cite-our-paper">How to cite our paper</h3> <p>Gabriela Kadlecová, Jovita Lukasik, Martin Pilát, Petra Vidnerová, Mahmoud Safari, Roman Neruda, &amp; Frank Hutter (2024). Surprisingly Strong Performance Prediction with Neural Graph Features. In Forty-first International Conference on Machine Learning.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{
kadlecov{\'a}2024surprisingly,
title={Surprisingly Strong Performance Prediction with Neural Graph Features},
author={Gabriela Kadlecov{\'a} and Jovita Lukasik and Martin Pil{\'a}t and Petra Vidnerov{\'a} and Mahmoud Safari and Roman Neruda and Frank Hutter},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=EhPpZV6KLk}
}
</code></pre></div></div> <h3 id="prerequisites">Prerequisites</h3> <p>For reading this blog post, it might be useful to know the basics of zero-cost proxies. You can read <a href="https://iclr-blog-track.github.io/2022/03/25/zero-cost-proxies/" rel="external nofollow noopener" target="_blank">a blog post</a> [<a href="#zcpblog">2</a>] or look at the NAS-Bench-Suite-Zero paper [<a href="#nbsuitezero">1</a>].</p> <h3 id="motivation">Motivation</h3> <p>Zero-cost proxies have quickly gained in popularity as fast-to-compute scores that correlate with network performance. This enabled performance estimation without any network training at the cost of few minibatch passes.</p> <p>Previous works discussed the strengths and weaknesses of zero-cost proxies (ZCP) [<a href="#nbsuitezero">1</a>, <a href="#zcpblog">2</a>]. As ZCP do not require network trainings, they provide very fast estimates out of the box. They can also be used in model-based prediction, including in transfer of the predictors to new datasets [<a href="#multipredict">3</a>, <a href="#robustzcp">17</a>].</p> <p>The flip side is that the ZCP scores have inconsistent correlation across search spaces – they can be great on NB201 and CIFAR-10, but poor on NB301 and CIFAR-10 [<a href="#nb201">4</a>, <a href="#nb301">5</a>, <a href="#cifar">6</a>]. Also, it is poorly understood <em>why</em> they correlate with the performance, and they exhibit biases with network properties (e.g. operation count).</p> <p>In our work, we analyzed the biases of zero-cost proxies. We discovered that some zero-cost proxies not only correlate with operation counts, sometimes they are even directly proportional to the number of convolutions!</p> <p>Inspired by the findings, we proposed neural graph features (shortly GRAF) that describe the structure of the cell and enable interpretable state-of-the-art performance prediction.</p> <p>Let’s delve into the details!</p> <h2 id="zero-cost-proxy-shortcomings">Zero-cost proxy shortcomings</h2> <p>To better understand why zero-cost proxies (ZCP) correlate so well with the validation accuracy, we want to look at scores and validation accuracies of many networks from a single search space. Conveniently, NAS-Bench-Suite-Zero (NB-Suite-Zero) contains precomputed ZCP for multiple benchmarks and tasks [<a href="#nbsuitezero">1</a>].</p> <p>We first look at NAS-Bench-201 (NB201) and CIFAR-10 [<a href="#nb201">4</a>, <a href="#cifar">6</a>] – a frequently used benchmark and dataset pair, where multiple proxies have a notably large correlation with the validation accuracy. NB-Suite-Zero contains ZCP scores of all possible architectures from the search space.</p> <p>In the figure below, we plot the NASWOT (denoted <code class="language-plaintext highlighter-rouge">nwot</code> [<a href="#naswot">8</a>]) scores against validation accuracy. We see that some clusters are formed – what could they represent?</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_graf/onlyconvcifar-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_graf/onlyconvcifar-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_graf/onlyconvcifar-1400.webp"></source> <img src="/assets/img/blog_graf/onlyconvcifar.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We examined networks in individual clusters, and we saw a pattern – all networks contained the same number of convolutions! We plot the scores again, this time coloring the networks by the number of <code class="language-plaintext highlighter-rouge">Conv1x1</code> + <code class="language-plaintext highlighter-rouge">Conv3x3</code> in the architecture:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_graf/nb201_cifar10_nwot_nconvs_new-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_graf/nb201_cifar10_nwot_nconvs_new-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_graf/nb201_cifar10_nwot_nconvs_new-1400.webp"></source> <img src="/assets/img/blog_graf/nb201_cifar10_nwot_nconvs_new.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The clusters could also be divided in half based on whether there is more <code class="language-plaintext highlighter-rouge">Conv3x3</code> or <code class="language-plaintext highlighter-rouge">Conv1x1</code>.</p> <p>The behavior is even more pronounced for <code class="language-plaintext highlighter-rouge">l2_norm</code> [<a href="#nbsuitezero">1</a>]:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_graf/nb201_l2_norm_nconvs_new-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_graf/nb201_l2_norm_nconvs_new-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_graf/nb201_l2_norm_nconvs_new-1400.webp"></source> <img src="/assets/img/blog_graf/nb201_l2_norm_nconvs_new.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The ZCPdo not exhibit these properties on NB301 (based on DARTS), and on NB101, the regularities are smaller. The finding is however still important given how often NB201 is used for the evaluation of NAS algorithms or new zero-cost proxies. We show more examples at the <a href="#bonus">bottom of the blog post</a>.</p> <h2 id="graf">GRAF</h2> <p>As we saw in the previous section, the number of convolutions has a notable correlation with the CIFAR-10 validation accuracy. This made us think – what if we collect other graph properties and use them along with the operation counts in model-based prediction? How good will the prediction be?</p> <p>We collected the following neural graph features (GRAF):</p> <ul> <li>operation counts</li> <li>minimum path length over operations \(O\)</li> <li>maximum path length over operations \(O\)</li> <li>input/output node degree counting operations \(O\)</li> <li>average input/output node degree across all nodes (counting \(O\))</li> </ul> <p>\(O\) is a subset of the full operation set \(\mathbb{O}\), and we compute them <em>for all</em> subsets of \(\mathbb{O}\).</p> <p><strong>Example features</strong>:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">max_path_(conv1x1, conv3x3)</code> (maximum path over convolutions)</li> <li> <code class="language-plaintext highlighter-rouge">min_path_(skip)</code> (existence of shortcuts - is there a skip to the output?)</li> <li> <code class="language-plaintext highlighter-rouge">node_degree_(conv3x3)_in_degree</code> (how many <code class="language-plaintext highlighter-rouge">conv3x3</code> are connected to the input?)</li> </ul> <p>We can use these features as an <em>encoding</em> of the networks from the search space, and use it for model-based prediction. First, we sample \(N\) networks for the train set, and compute their GRAF features. Then, we fit a random forest predictor on the features and corresponding targets (e.g. validation accuracy).</p> <p>We compare the GRAF encoding also with the set of all ZCP from NB-Suite-Zero, and with the one-hot encoding – one-hot encoded operations along with the flattened adjacency matrix – and with their combinations.</p> <p>Let’s look at the results for NB101, NB201 and NB301 [<a href="#nb101">9</a>, <a href="#nb201">4</a>, <a href="#nb301">5</a>] (all for CIFAR-10) and 1024 train networks (all encodings that contain GRAF are in blue):</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_graf/cifar10-NBx01_1024.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_graf/cifar10-NBx01_1024.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_graf/cifar10-NBx01_1024.svg-1400.webp"></source> <img src="/assets/img/blog_graf/cifar10-NBx01_1024.svg" class="img-fluid rounded z-depth-1" width="200%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We see that GRAF outperforms both ZCP and one-hot, and the combination of ZCP + GRAF has the overall best results!</p> <p>GRAF also performs well on TransNAS-Bench-101 micro tasks, for example on the autoencoder task (sample sizes 32, 128, 1024) [<a href="#tnb">14</a>]:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_graf/tnb101_micro-small-1-crop.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_graf/tnb101_micro-small-1-crop.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_graf/tnb101_micro-small-1-crop.svg-1400.webp"></source> <img src="/assets/img/blog_graf/tnb101_micro-small-1-crop.svg" class="img-fluid rounded z-depth-1" width="200%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Results on other tasks and sample sizes are included in the paper. Also, we present there a variant of GRAF for Trans-NAS-Bench-101 macro [<a href="#tnb">14</a>].</p> <p><strong>In the next section, we will mostly use GRAF and ZCP together (as they perform the best when used jointly).</strong></p> <h2 id="interpretable-prediction">Interpretable prediction</h2> <p>The good performance is not the only thing where GRAF shines – since the features are intuitive , and we use a tabular predictor, we can leverage interpretability techniques like SHAP [<a href="#shap">10</a>].</p> <p>For every feature and ZCP, we compute the SHAP feature importance, and compute mean rank over 10 runs. Then, we return 10 ZCP or GRAF with the lowest rank:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_graf/explaaain-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_graf/explaaain-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_graf/explaaain-1400.webp"></source> <img src="/assets/img/blog_graf/explaaain.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We see that features and ZCP that are the most important for CIFAR-10 prediction are quite different from those that are influential on the <code class="language-plaintext highlighter-rouge">autoencoder</code> task.</p> <h2 id="comparison-with-existing-methods">Comparison with existing methods.</h2> <p>To compare GRAF with existing predictors, we followed the same setup as in the performance predictor survey [<a href="#perfpred">11</a>]. We use the ZCP + GRAF variant as it had the best performance, and also include an XGBoost predictor.</p> <p>The first experiment was performance prediction across various train net sample sizes. Here, we show the results on NB101 and CIFAR-10 [<a href="#nb101">9</a>, <a href="#cifar">6</a>]:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_graf/train_nb101.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_graf/train_nb101.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_graf/train_nb101.svg-1400.webp"></source> <img src="/assets/img/blog_graf/train_nb101.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>ZCP + GRAF outperformed all predictors across all sample sizes, including complex predictors like graph neural networks or SemiNAS – a semi-supervised LSTM predictor [<a href="#seminas">12</a>].</p> <p>The second experiments compared predictors in search settings – they were used as surrogates in Bayesian optimization (refer to our paper for more details). We show the results for NB201 and ImageNet16-120 [<a href="#nb201">3</a>, <a href="#imagenet">7</a>]:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_graf/nasbench201_ImageNet16-120_total_time.svg-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_graf/nasbench201_ImageNet16-120_total_time.svg-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_graf/nasbench201_ImageNet16-120_total_time.svg-1400.webp"></source> <img src="/assets/img/blog_graf/nasbench201_ImageNet16-120_total_time.svg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>ZCP + GRAF is more sample efficient than all the other predictors.</p> <p>It is important to note that some models still outperform ZCP + GRAF in some settings. SemiNAS performs on par with ZCP + GRAF on NB201 + CIFAR-10 search [<a href="#seminas">12</a>, <a href="#nb201">3</a>, <a href="#cifar">6</a>]. BRP-NAS – a graph neural network (GNN) predictor – can be combined with ZCP + GRAF for best results on NB301 [<a href="#brpnas">13</a>, <a href="#nb301">5</a>]. However, in most cases the GNN models perform the same as ZCP + GRAF + random forest, while taking longer to train.</p> <h2 id="conclusion">Conclusion</h2> <p>In this blog post, we introduced GRAF – neural graph features that enable interpretable tabular performance prediction. We highlighted the main ideas along with its strengths and results.</p> <p>The main weakness of GRAF is that it has to be devised every time a new search space is designed. For most graph-based search spaces, the general idea of using paths, node degrees and operation counts can be easily applied (although due to different operation sets, transfer is not possible). It is also possible to use <a href="#gpt">large language models</a> for new GRAF proposal, notably for macro search spaces.</p> <p>Overall, we see GRAF as an important baseline for more complex method and a valuable interpretability tool for neural architecture search.</p> <p>More details can be found in our paper</p> <ul> <li>even more results across popular search spaces</li> <li>macro search space features</li> <li>longer discussion</li> <li>results when combined with BRP-NAS - a graph neural network predictor</li> <li>results on hardware tasks (energy, latency) and robustness tasks [<a href="#hw">15</a>, <a href="#robust">16</a>]</li> </ul> <p>[1] <a name="nbsuitezero"></a> Arjun Krishnakumar, Colin White, Arber Zela, Renbo Tu, Mahmoud Safari, &amp; Frank Hutter (2022). NAS-Bench-Suite-Zero: Accelerating Research on Zero Cost Proxies. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.</p> <p>[2] <a name="zcpblog"></a> White, C., Khodak, M., Tu, R., Shah, S., Bubeck, S., &amp; Dey, D. (2022). A Deeper Look at Zero-Cost Proxies for Lightweight NAS. In ICLR Blog Track.</p> <p>[3] <a name="multipredict"></a> Yash Akhauri, &amp; Mohamed S Abdelfattah (2023). Multi-Predict: Few Shot Predictors For Efficient Neural Architecture Search. In AutoML Conference 2023.</p> <p>[4] <a name="nb201"></a> Dong, X., &amp; Yang, Y. (2020). NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search. In International Conference on Learning Representations (ICLR).</p> <p>[5] <a name="nb301"></a> Arber Zela, Julien Niklas Siems, Lucas Zimmer, Jovita Lukasik, Margret Keuper, &amp; Frank Hutter (2022). Surrogate NAS Benchmarks: Going Beyond the Limited Search Spaces of Tabular NAS Benchmarks. In International Conference on Learning Representations.</p> <p>[6] <a name="cifar"></a> Alex Krizhevsky, Learning Multiple Layers of Features from Tiny Images, 2009.</p> <p>[7] <a name="imagenet"></a> Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., &amp; Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition (pp. 248–255).</p> <p>[8] <a name="naswot"></a> Joseph Mellor, Jack Turner, Amos Storkey, &amp; Elliot J. Crowley (2021). Neural Architecture Search without Training. In International Conference on Machine Learning.</p> <p>[9] <a name="nb101"></a> Ying, C., Klein, A., Christiansen, E., Real, E., Murphy, K., &amp; Hutter, F. (2019). NAS-Bench-101: Towards Reproducible Neural Architecture Search. In Proceedings of the 36th International Conference on Machine Learning (pp. 7105–7114). PMLR.</p> <p>[10] <a name="shap"></a> Lundberg, S., &amp; Lee, S.I. (2017). A Unified Approach to Interpreting Model Predictions. In Advances in Neural Information Processing Systems 30 (pp. 4765–4774). Curran Associates, Inc..</p> <p>[11] <a name="perfpred"></a> White, C., Zela, A., Ru, R., Liu, Y., &amp; Hutter, F. (2021). How Powerful are Performance Predictors in Neural Architecture Search?. In Advances in Neural Information Processing Systems (pp. 28454–28469). Curran Associates, Inc..</p> <p>[12] <a name="seminas"></a> Luo, R., Tan, X., Wang, R., Qin, T., Chen, E., &amp; Liu, T.Y. (2020). Semi-Supervised Neural Architecture Search. In Advances in Neural Information Processing Systems (pp. 10547–10557). Curran Associates, Inc..</p> <p>[13] <a name="brpnas"></a> Dudziak, Ł, Chau, T., Abdelfattah, M., Lee, R., Kim, H., &amp; Lane, N. (2020). BRP-NAS: prediction-based NAS using GCNs. In Proceedings of the 34th International Conference on Neural Information Processing Systems. Curran Associates Inc..</p> <p>[14] <a name="tnb"></a> Duan, Y., Chen, X., Xu, H., Chen, Z., Liang, X., Zhang, T., &amp; Li, Z. (2021). TransNAS-Bench-101: Improving Transferability and Generalizability of Cross-Task Neural Architecture Search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 5251–5260).</p> <p>[15] <a name="hw"></a> Chaojian Li, Zhongzhi Yu, Yonggan Fu, Yongan Zhang, Yang Zhao, Haoran You, Qixuan Yu, Yue Wang, Cong Hao, &amp; Yingyan Lin (2021). \HW-\NAS-Bench: Hardware-Aware Neural Architecture Search Benchmark. In International Conference on Learning Representations.</p> <p>[16] <a name="robust"></a> Steffen Jung, Jovita Lukasik, &amp; Margret Keuper (2023). Neural Architecture Design and Robustness: A Dataset. In The Eleventh International Conference on Learning Representations .</p> <p>[17] <a name="robustzcp"></a> Lukasik, J., Moeller, M., and Keuper, M. An evaluation of zero-cost proxies – from neural architecture performance to model robustness. In DAGM German Conference on Pattern Recognition, 2023.</p> <h2 id="bonus-material">Bonus material</h2> <p><a name="bonus"></a></p> <h3 id="zero-cost-proxies-and-the-number-of-convolutions">Zero-cost proxies and the number of convolutions</h3> <p>When we again plot nwot by validation accuracy for NB201, this time for <strong>ImageNet16-120</strong> [<a href="#imagenet">7</a>], we can see <em>why</em> nwot has a lower correlation on this dataset - the distribution of validation accuracies is more stretched.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_graf/nb201_ImageNet16-120_nwot_nconvs_new-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_graf/nb201_ImageNet16-120_nwot_nconvs_new-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_graf/nb201_ImageNet16-120_nwot_nconvs_new-1400.webp"></source> <img src="/assets/img/blog_graf/nb201_ImageNet16-120_nwot_nconvs_new.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Finally, we see why zero-cost proxies have a poor performance on <strong>TransNAS-Bench-101 micro</strong> tasks – the validation accuracy does not depend on the number of convolutions as on CIFAR-10 (here plotted for the <code class="language-plaintext highlighter-rouge">class_scene</code> task) [<a href="#tnb">14</a>].</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_graf/tnb101_class_scene_nwot_nconvs_new-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_graf/tnb101_class_scene_nwot_nconvs_new-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_graf/tnb101_class_scene_nwot_nconvs_new-1400.webp"></source> <img src="/assets/img/blog_graf/tnb101_class_scene_nwot_nconvs_new.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>For <strong>NB301</strong> (a subset of darts), no clusters are formed for the ZCP (below we plot nwot by CIFAR-10 validation accuracy):</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_graf/nb301_nwot-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_graf/nb301_nwot-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_graf/nb301_nwot-1400.webp"></source> <img src="/assets/img/blog_graf/nb301_nwot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>However, the number of <strong>separable convolutions</strong> is a competitive predictor:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_graf/nb301_sepconvs-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_graf/nb301_sepconvs-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_graf/nb301_sepconvs-1400.webp"></source> <img src="/assets/img/blog_graf/nb301_sepconvs.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="using-chatgpt-for-feature-proposal-">Using ChatGPT for feature proposal <a name="gpt"></a> </h3> <p>During the reviewing process, one of the reviewers asked about potential automatic discovery of features – if a new search space differs from the previous spaces, it is necessary to design new GRAF (macro) features.</p> <p>To address the question, we briefly experimented with GPT4 to obtain suggestions for macro features on an unexplored (hypothetical) transformer search space. The outputs look promisingAn excerpt of our experiment is listed here:</p> <blockquote> <p><strong>Prompt:</strong> What could be useful characteristics of neural architectures that can be computed syntactically and that are predictive of the architecture’s performance?</p> </blockquote> <p>(… ommiting outputs related to convnets …)</p> <blockquote> <p><strong>Prompt:</strong> How about transformers?</p> </blockquote> <p>ChatGPT output:</p> <blockquote> <p>Transformers, initially introduced for natural language processing tasks, have shown remarkable success across a wide range of domains including image processing, audio signal processing, and more. Unlike convolutional neural networks (CNNs) that process data sequentially and locally, transformers use self-attention mechanisms to weigh the importance of different parts of the input data globally. For transformer architectures, several syntactic characteristics can be computed and are predictive of the architecture’s performance:</p> <ol> <li> <strong>Number of Transformer Layers (Depth)</strong>: This refers to how many transformer blocks or encoder/decoder pairs are stacked. More layers allow the network to learn more complex relationships at the cost of increased computational complexity and potential challenges in training stability.</li> <li> <strong>Model Dimensionality (d_model)</strong>: The size of the embeddings and the hidden layers within each transformer block. Higher dimensionality can increase the capacity of the model to learn complex relationships but also increases the number of parameters and computational requirements.</li> <li> <strong>Number of Attention Heads (h)</strong>: Transformer models use multi-head attention mechanisms to allow the model to focus on different parts of the input for different reasons simultaneously. The number of heads determines the width of the model’s attention and influences the model’s ability to capture various aspects of the data.</li> <li> <strong>Size of the Feed-Forward Networks</strong>: Each transformer block contains a position-wise feed-forward network. The size of these networks (often denoted as d_ff in literature) is a critical parameter that affects the model’s capacity and its computational demands.</li> <li> <strong>Vocabulary Size</strong>: For models trained on textual data, the size of the vocabulary affects the embedding layer’s size and thus the number of parameters. For tasks involving other types of data, analogous parameters might define the input feature space’s dimensionality.</li> <li> <strong>Sequence Length</strong>: The maximum sequence length that the model can handle. This impacts the computational complexity, especially in the self-attention mechanism, where the complexity grows quadratically with the sequence length.</li> <li> <strong>Positional Encoding Scheme</strong>: Transformers use positional encodings to incorporate information about the order of the sequence elements. The choice of positional encoding (learned embeddings vs. sinusoidal functions) does not add parameters but can influence performance.</li> <li> <strong>Attention Mechanism Variant</strong>: Beyond the standard scaled dot-product attention, there are variants like local attention, sparse attention, or the recently popular efficient attention mechanisms that reduce computational complexity. The choice and configuration of the attention mechanism can significantly impact model performance and efficiency.</li> <li> <strong>Regularization Techniques</strong>: Identifying the presence of regularization techniques such as dropout, attention dropout, and layer normalization within the transformer. These techniques are crucial for preventing overfitting and ensuring stable training.</li> <li> <strong>Parameter Initialization Scheme</strong>: Transformers are sensitive to the initialization of parameters. The chosen scheme (e.g., Xavier, He initialization) can significantly impact the training dynamics and the final model performance.</li> <li> <strong>Computational Complexity</strong>: Estimating the computational complexity, especially in terms of FLOPs (floating-point operations) for the self-attention mechanism and the overall forward pass. This complexity impacts training time, inference speed, and the feasibility of deploying the model in resource-constrained environments.</li> </ol> <p>These characteristics provide a framework for understanding, comparing, and optimizing transformer architectures. They influence the model’s learning capacity, computational efficiency, and applicability to different tasks and datasets.</p> </blockquote> </div> </article><div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"gabikadlecova/gabikadlecova.github.io","data-repo-id":"R_kgDOKZz1gQ","data-category":"General","data-category-id":"DIC_kwDOKZz1gc4CZvDK","data-mapping":"pathname","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Gabi Kadlecová. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> - <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme; <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photo by <a href="https://www.instagram.com/michalpospech/" rel="external nofollow noopener" target="_blank">Michal Pospěch</a>. Last updated: June 08, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>